{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded78a87",
   "metadata": {},
   "source": [
    "# Company Profile EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c495b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer,RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "!pip install lda\n",
    "import lda\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d239d40",
   "metadata": {},
   "source": [
    "## Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c21fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_jobs = pd.read_csv('fake_job_postings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e347e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_jobs.head()\n",
    "fake_jobs['company_profile']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9118dd",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(textData):\n",
    "    stop_words_list = stopwords.words('english') # Choose the stop English stopwords\n",
    "    stop_words_list.append('–')# Add - to the stopwords\n",
    "    stop_words = set(stop_words_list)\n",
    "    word_tokens = [t for t in textData.split()] # Split the comment into words\n",
    "    # For every word, return the word if it is not a stopword, a doubl-quote, a dash, a comma or a period.\n",
    "    filtered_sentence = [w for w in word_tokens if (not w == '\"') & (not w == '-') & (not w == '.') & (not w == ',') & (not w.lower() in stop_words)]\n",
    "    return filtered_sentence\n",
    "\n",
    "def filteredFreq(postList):\n",
    "    wordFrecSeries = [] # Create empty list\n",
    "    wordFrecSeries = pd.Series(wordFrecSeries) # Convert the list into a Series\n",
    "    # For each post in the list of posts (the parameter) get the frequency of each of its words\n",
    "    for post in postList:\n",
    "        # For each word in the post, get its frequency\n",
    "        for word in post:\n",
    "            # If the word had already been found sum 1, else add the word to the Series\n",
    "            if word in wordFrecSeries:\n",
    "                wordFrecSeries[word] = wordFrecSeries[word]+1\n",
    "            else:\n",
    "                wordFrecSeries[word] = 1\n",
    "    return wordFrecSeries.sort_values(ascending=False)[:400] # Order the series and return the top 400 values\n",
    "\n",
    "company_profile = fake_jobs['company_profile'].astype(str)\n",
    "word_list = company_profile.map(removeStopwords)\n",
    "frequency_series = filteredFreq(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c720561",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frequency_series[:50])\n",
    "#print(frequency_series[51:100])\n",
    "#print(frequency_series[101:150])\n",
    "print('Important Attributes: services, technology, platform, sofwtare, communications, marketing, education, design, startup, HR')\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(frequency_series[:25].index, frequency_series[:25], color='mediumaquamarine')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# fake_jobs['company_profile'][fake_jobs['company_profile'] == 'nan'] MADE SURE THERE WERE NO FALSE nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad435774",
   "metadata": {},
   "source": [
    "## Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03690ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(textData):\n",
    "    stop_words_list = stopwords.words('english') # Choose the stop English stopwords\n",
    "    stop_words_list.append('–')# Add - to the stopwords\n",
    "    stop_words = set(stop_words_list)\n",
    "    word_tokens = [t for t in textData.split()] # Split the comment into words\n",
    "    # For every word, return the word if it is not a stopword, a doubl-quote, a dash, a comma or a period.\n",
    "    filtered_sentence = [w for w in word_tokens if (not w == '\"') & (not w == '-') & (not w == '.') & (not w == ',') & (not w.lower() in stop_words)]\n",
    "    return filtered_sentence\n",
    "\n",
    "def getLiftAB(termA, termB, postList):\n",
    "    \"\"\"\n",
    "    Function that calculates the lift between two terms\n",
    "    \"\"\"\n",
    "    flagInPostA = 0\n",
    "    flagInPostB = 0\n",
    "    flagInPostAB = 0\n",
    "    freqAB = 0\n",
    "    freqA = 0\n",
    "    freqB = 0\n",
    "    for post in postList: # For each post in the list of posts (the parameter) get the frequency of each of its words\n",
    "        for word in post: # For each word in the post, get its frequency and then set the flags back to the original value\n",
    "            if flagInPostAB == 0: # If both terms have not been found inside the post\n",
    "                if flagInPostA == 0: # If term A has not been previously found in the post check if the current word is term A\n",
    "                    if word == termA: # If the current word is term A add 1 to the frequency and set the flagA to 1\n",
    "                        freqA += 1\n",
    "                        flagInPostA = 1\n",
    "                if flagInPostB == 0: # If term B has not been previously found in the post check if the current word is term B\n",
    "                    if word == termB: # If the current word is term B add 1 to the frequency and set the flagB to 1\n",
    "                        freqB += 1\n",
    "                        flagInPostB = 1\n",
    "                if (flagInPostA == 1) & (flagInPostB == 1): # If after this iteration both terms have been found inside the post, add 1 to the frequncy AB and set the flagAB to 1\n",
    "                    freqAB += 1\n",
    "                    flagInPostAB = 1\n",
    "        # After all words have been checked, return the flags to zero and move on to the next post\n",
    "        flagInPostA = 0\n",
    "        flagInPostB = 0\n",
    "        flagInPostAB = 0\n",
    "    # Make sure that the lifts can be computed (no divisions over zero)\n",
    "    if freqA == 0:\n",
    "        freqA = 1\n",
    "    if freqB == 0:\n",
    "        freqB = 1\n",
    "    # Compute lift between term A and B\n",
    "    liftAB = (len(postList) * freqAB) / (freqA * freqB)\n",
    "    return liftAB  \n",
    "\n",
    "def liftTable(importantList, postList):\n",
    "    \"\"\"\n",
    "    Function that gets the lifts between many words (the ones with highest frequncies)\n",
    "    \"\"\"\n",
    "    liftList = []\n",
    "    liftSeries = pd.Series(liftList) # Create empty series\n",
    "    liftDataFrame = pd.DataFrame(liftSeries) # Create empty dataframe\n",
    "    for i in range(len(importantList)): # For each brand\n",
    "        liftDataFrame.loc[importantList[i], importantList[i]] = 0\n",
    "        for j in range(i+1,len(importantList)): # For each pair of brands\n",
    "            liftAB = getLiftAB(importantList[i], importantList[j], postList) # Get the lift of this brand-pair\n",
    "            liftDataFrame.loc[importantList[i], importantList[j]] = liftAB\n",
    "            liftDataFrame.loc[importantList[j], importantList[i]] = liftAB\n",
    "    liftDataFrame = liftDataFrame.drop([0], axis=1)\n",
    "    return liftDataFrame\n",
    "\n",
    "company_profile = fake_jobs['company_profile'].dropna().astype(str) # Get data\n",
    "word_list = company_profile.map(removeStopwords) # For the company profile column, map the function removeStopWords\n",
    "attribute_list = ['services', 'technology', 'platform', 'sofwtare', 'communications', 'marketing', 'education', 'design', 'startup', 'HR']\n",
    "lift_table = liftTable(attribute_list, word_list)\n",
    "print(lift_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb506b",
   "metadata": {},
   "source": [
    "## MDS Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a60fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source1: https://stackabuse.com/guide-to-multidimensional-scaling-in-python-with-scikit-learn/\n",
    "# Source2: https://stackoverflow.com/questions/14432557/scatter-plot-with-different-text-at-each-data-point\n",
    "\n",
    "def removeStopwords(textData):\n",
    "    stop_words_list = stopwords.words('english') # Choose the stop English stopwords\n",
    "    stop_words_list.append('–')# Add - to the stopwords\n",
    "    stop_words = set(stop_words_list)\n",
    "    word_tokens = [t for t in textData.split()] # Split the comment into words\n",
    "    # For every word, return the word if it is not a stopword, a doubl-quote, a dash, a comma or a period.\n",
    "    filtered_sentence = [w for w in word_tokens if (not w == '\"') & (not w == '-') & (not w == '.') & (not w == ',') & (not w.lower() in stop_words)]\n",
    "    return filtered_sentence\n",
    "\n",
    "def getLiftAB(termA, termB, postList):\n",
    "    \"\"\"\n",
    "    Function that calculates the lift between two terms (brands)\n",
    "    \"\"\"\n",
    "    flagInPostA = 0\n",
    "    flagInPostB = 0\n",
    "    flagInPostAB = 0\n",
    "    freqAB = 0\n",
    "    freqA = 0\n",
    "    freqB = 0\n",
    "    for post in postList: # For each post in the list of posts (the parameter) get the frequency of each of its words\n",
    "        for word in post: # For each word in the post, get its frequency and then set the flags back to the original value\n",
    "            if flagInPostAB == 0: # If both terms have not been found inside the post\n",
    "                if flagInPostA == 0: # If term A has not been previously found in the post check if the current word is term A\n",
    "                    if word == termA: # If the current word is term A add 1 to the frequency and set the flagA to 1\n",
    "                        freqA += 1\n",
    "                        flagInPostA = 1\n",
    "                if flagInPostB == 0: # If term B has not been previously found in the post check if the current word is term B\n",
    "                    if word == termB: # If the current word is term B add 1 to the frequency and set the flagB to 1\n",
    "                        freqB += 1\n",
    "                        flagInPostB = 1\n",
    "                if (flagInPostA == 1) & (flagInPostB == 1): # If after this iteration both terms have been found inside the post, add 1 to the frequncy AB and set the flagAB to 1\n",
    "                    freqAB += 1\n",
    "                    flagInPostAB = 1\n",
    "        # After all words have been checked, return the flags to zero and move on to the next post\n",
    "        flagInPostA = 0\n",
    "        flagInPostB = 0\n",
    "        flagInPostAB = 0\n",
    "    # Make sure that the lifts can be computed (no divisions over zero)\n",
    "    if freqA == 0:\n",
    "        freqA = 1\n",
    "    if freqB == 0:\n",
    "        freqB = 1\n",
    "    # Compute lift between term A and B\n",
    "    liftAB = (len(postList) * freqAB) / (freqA * freqB)\n",
    "    return liftAB  \n",
    "\n",
    "def liftTable(importantList, postList):\n",
    "    \"\"\"\n",
    "    Function that gets the lifts between many words (the ones with highest frequncies)\n",
    "    \"\"\"\n",
    "    liftList = []\n",
    "    liftSeries = pd.Series(liftList) # Create empty series\n",
    "    liftDataFrame = pd.DataFrame(liftSeries) # Create empty dataframe\n",
    "    for i in range(len(importantList)): # For each brand\n",
    "        liftDataFrame.loc[importantList[i], importantList[i]] = 0\n",
    "        for j in range(i+1,len(importantList)): # For each pair of brands\n",
    "            liftAB = getLiftAB(importantList[i], importantList[j], postList) # Get the lift of this brand-pair\n",
    "            liftDataFrame.loc[importantList[i], importantList[j]] = liftAB\n",
    "            liftDataFrame.loc[importantList[j], importantList[i]] = liftAB\n",
    "    liftDataFrame = liftDataFrame.drop([0], axis=1)\n",
    "    return liftDataFrame\n",
    "\n",
    "def inverseLift(importantList, postList):\n",
    "    \"\"\"\n",
    "    Function that gets the lifts between many brands (the ones with highest frequncies)\n",
    "    \"\"\"\n",
    "    liftList = []\n",
    "    liftSeries = pd.Series(liftList) # Create empty series\n",
    "    liftDataFrame = pd.DataFrame(liftSeries) # Create empty dataframe\n",
    "    for i in range(len(importantList)): # For each brand\n",
    "        liftDataFrame.loc[importantList[i], importantList[i]] = 0\n",
    "        for j in range(i+1,len(importantList)): # For each pair of brands\n",
    "            liftAB = getLiftAB(importantList[i], importantList[j], postList) # Get the lift of this brand-pair\n",
    "            if liftAB == 0:\n",
    "                inverseLift = 8\n",
    "            else:\n",
    "                inverseLift = 1/liftAB\n",
    "            liftDataFrame.loc[importantList[i], importantList[j]] = inverseLift\n",
    "            liftDataFrame.loc[importantList[j], importantList[i]] = inverseLift\n",
    "    liftDataFrame = liftDataFrame.drop([0], axis=1)\n",
    "    return liftDataFrame\n",
    "\n",
    "word_list = fake_jobs['company_profile'].dropna().astype(str).map(removeStopwords) # Get data\n",
    "attribute_list = ['services', 'technology', 'platform', 'sofwtare', 'communications', 'marketing', 'education', 'design', 'startup', 'HR']\n",
    "attribute_lifts = inverseLift(attribute_list, word_list)\n",
    "mdsAttributes = MDS(random_state=0)\n",
    "liftTransform = mdsAttributes.fit_transform(attribute_lifts) # Transform distances into 2D\n",
    "colors = ['lightcoral', 'yellowgreen', 'teal', 'orangered', 'gold', 'forestgreen', 'firebrick', 'maroon', 'goldenrod', 'darkviolet']\n",
    "plt.scatter(liftTransform[:,0], liftTransform[:,1], c=colors)#, s=size, c=colors)\n",
    "plt.title('MDS Plot')\n",
    "for i, txt in enumerate(attribute_list):\n",
    "    plt.annotate(txt, (liftTransform[:,0][i], liftTransform[:,1][i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9799f7f5",
   "metadata": {},
   "source": [
    "## Creat Company ID Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fff43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_id = []\n",
    "for i in range(len(fake_jobs['company_profile'].value_counts())):\n",
    "    company_id.append('company_{}'.format(i))\n",
    "company_id_series = pd.Series(company_id, fake_jobs['company_profile'].value_counts().index)\n",
    "\n",
    "def getCompanyID(company_profile):\n",
    "    return company_id_series[company_profile]\n",
    "\n",
    "company_profile_LDA = fake_jobs.copy()\n",
    "company_profile_LDA = company_profile_LDA[company_profile_LDA['company_profile'].notnull()]\n",
    "company_profile_LDA['company_id'] = company_profile_LDA['company_profile'].map(getCompanyID)\n",
    "company_profile_LDA[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b591d",
   "metadata": {},
   "source": [
    "## Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3caab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: Prof. Barua\n",
    "\n",
    "fakeJob_id = input('provide the column name for id: ') # job_id\n",
    "company_id = input('provide the column name for company id: ') # company_id\n",
    "company_profile = input('provide the column name for text: ') # company_profile\n",
    "ntopics= input('Provide the number of latent topics: ');\n",
    "\n",
    "\n",
    "word_tokenizer=RegexpTokenizer(r'\\w+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords_nltk = stopwords.words('english') # Choose the stop English stopwords\n",
    "extraStopWords = ['–']\n",
    "for word in extraStopWords: # Add – to the stopwords\n",
    "    stopwords_nltk.append(word)\n",
    "stopwords_nltk = set(stopwords_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(version_desc):\n",
    "    lowercase=version_desc.lower()\n",
    "    text = wordnet_lemmatizer.lemmatize(lowercase)\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "vec_words = CountVectorizer(tokenizer=tokenize_text,stop_words=stopwords_nltk,decode_error='ignore')\n",
    "total_features_words = vec_words.fit_transform(company_profile_LDA[company_profile])\n",
    "\n",
    "print(total_features_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lda.LDA(n_topics=int(ntopics), n_iter=300, random_state=1) # CHANGE n_iter BACK TO 500\n",
    "model.fit(total_features_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65707347",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = model.topic_word_\n",
    "doc_topic=model.doc_topic_\n",
    "doc_topic=pd.DataFrame(doc_topic)\n",
    "company_profile_LDA=company_profile_LDA.join(doc_topic)\n",
    "jobs=pd.DataFrame()\n",
    "\n",
    "for i in range(int(ntopics)):\n",
    "    topic=\"topic_\"+str(i)\n",
    "    jobs[topic]=company_profile_LDA.groupby([company_id])[i].mean()\n",
    "\n",
    "jobs=jobs.reset_index()\n",
    "topics=pd.DataFrame(topic_word)\n",
    "topics.columns=vec_words.get_feature_names()\n",
    "topics1=topics.transpose()\n",
    "print (\"Topics word distribution written in file AMLproject_topic_word_dist.xlsx \")\n",
    "topics1.to_excel(\"AMLproject_topic_word_dist.xlsx\")\n",
    "jobs.to_excel(\"AMLproject_document_topic_dist.xlsx\",index=False)\n",
    "print (\"Document topic distribution written in file AMLproject_document_topic_dist.xlsx \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7c282",
   "metadata": {},
   "source": [
    "## LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb89f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "companyTopics = pd.read_excel(\"AMLproject_document_topic_dist.xlsx\")\n",
    "\n",
    "wordTopics = pd.read_excel(\"AMLproject_topic_word_dist.xlsx\")\n",
    "\n",
    "wordTopics.rename(columns={'Unnamed: 0': 'word'}, inplace=True)\n",
    "wordTopics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTopics.sort_values(by=0, ascending=False)[:20] # general\n",
    "wordTopics.sort_values(by=2, ascending=False)[:20] #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42af973",
   "metadata": {},
   "source": [
    "## Company ID Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_id = []\n",
    "for i in range(len(fake_jobs['company_profile'].value_counts())):\n",
    "    company_id.append('company_{}'.format(i))\n",
    "company_id_series = pd.Series(company_id, fake_jobs['company_profile'].value_counts().index)\n",
    "\n",
    "def getCompanyID(company_profile):\n",
    "    return company_id_series[company_profile]\n",
    "\n",
    "company_profile_Cosine = fake_jobs.copy()[['job_id', 'company_profile', 'location']]\n",
    "company_profile_Cosine = company_profile_Cosine[company_profile_Cosine['company_profile'].notnull()]\n",
    "company_profile_Cosine['company_id'] = company_profile_Cosine['company_profile'].map(getCompanyID)\n",
    "company_profile_Cosine[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a93504",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings\n",
    "WORD = re.compile(r\"[^,-]\\w+\")\n",
    "\n",
    "def text_to_vector(text):\n",
    "    \"\"\"\n",
    "    Convert text to vector.\n",
    "    \"\"\"\n",
    "    if type(text) == list:\n",
    "        text1 = []\n",
    "        for word in text:\n",
    "            text1.append(word.replace(\"'\", \"\"))\n",
    "        text1 = ' '.join(text1).lower()\n",
    "        words = WORD.findall(text1)\n",
    "    else:\n",
    "        words = WORD.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Get the cosine similarity between attributes and reviews.\n",
    "    \"\"\"\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "# Get attributes from user\n",
    "attributes = str(input('What attributes are you looking for? (separate you attributes by commas): ')).lower()\n",
    "attributeList = attributes.split(', ')\n",
    "attributeDF = pd.DataFrame(attributeList)\n",
    "attributeDF.rename(columns={0: \"Attributes\"}, inplace=True)\n",
    "attributeDF.to_csv('CompanyAttributes.csv', header=True, index=False)\n",
    "attributeDF\n",
    "\n",
    "company_profile_Cosine # data\n",
    "to_compareFile = pd.read_csv('CompanyAttributes.csv') # Open attributes CSV\n",
    "to_compareText = ' '.join(list(to_compareFile['Attributes']))\n",
    "to_compareVector = text_to_vector(to_compareText)\n",
    "noStopWords = company_profile_Cosine['company_profile'].map(removeStopwords) # For the company profile map the function removeStopWords\n",
    "company_profileVector = noStopWords.map(text_to_vector)\n",
    "cosines = pd.Series([get_cosine(to_compareVector, company_profile) for company_profile in company_profileVector])\n",
    "company_profile_DF = pd.DataFrame(company_profile_Cosine['company_id'])\n",
    "company_profile_DF['company_profile'] = company_profile_Cosine['company_profile']\n",
    "company_profile_DF['company_profile'] = company_profile_DF['company_profile'].apply(lambda x: x.replace('\\n', ' '))\n",
    "company_profile_DF['Similarity'] = cosines\n",
    "company_profile_DF.sort_values(by='Similarity', ascending=False, inplace=True)        \n",
    "company_profile_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c0b4a",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000053ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
    "def get_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    Get the sentiments of a sentence.\n",
    "    \"\"\"\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    " \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # object gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "\n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        reviewSentiment = 'Positive: {}%'.format(sentiment_dict['pos']*100)\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        reviewSentiment = 'Negative: {}%'.format(sentiment_dict['neg']*100)\n",
    "    else :\n",
    "        reviewSentiment = 'Neutral: {}%'.format(sentiment_dict['neu']*100)\n",
    "    \n",
    "    return reviewSentiment\n",
    "\n",
    "company_profile_DF['Sentiment'] = company_profile_DF['company_profile'].map(get_sentiment) # For the company_profile map the function get_sentiment\n",
    "company_profile_DF.sort_values(by='Similarity')[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dbf59f",
   "metadata": {},
   "source": [
    "## Realizing Companies Are Either Fake or Not Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_id = []\n",
    "for i in range(len(fake_jobs['company_profile'].value_counts())):\n",
    "    company_id.append('company_{}'.format(i))\n",
    "company_id_series = pd.Series(company_id, fake_jobs['company_profile'].value_counts().index)\n",
    "\n",
    "def getCompanyID(company_profile):\n",
    "    return company_id_series[company_profile]\n",
    "\n",
    "FakeJobs = fake_jobs.copy()\n",
    "FakeJobs = FakeJobs[FakeJobs['company_profile'].notnull()]\n",
    "FakeJobs['company_id'] = FakeJobs['company_profile'].map(getCompanyID)\n",
    "\n",
    "print(FakeJobs['company_id'].value_counts())\n",
    "\n",
    "fraudulents = FakeJobs[FakeJobs['fraudulent'] == 1]\n",
    "NOTfraudulents = FakeJobs[FakeJobs['fraudulent'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in fraudulents['company_id'].value_counts().index:\n",
    "    if company in NOTfraudulents['company_id'].value_counts().index:\n",
    "        print('In Not Fake')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
